{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'driver_config' from 'c:\\\\Users\\\\ckelaid\\\\Documents\\\\Scraping\\\\driver_config.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import math\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import pickle\n",
    "import GetSalary\n",
    "import importlib\n",
    "import scrape2_2 as scr2\n",
    "import driver_config as dc\n",
    "import numpy as np\n",
    "import scipy.interpolate as si\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "importlib.reload(scr2)\n",
    "importlib.reload(dc)\n",
    "\n",
    "# Remove DevTools warning\n",
    "#chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def human_like_mouse(action:ActionChains, startElem):\n",
    "    # B-spline function\n",
    "    # Curve base:\n",
    "    points = [[0, 0], [0, 2], [2, 3], [4, 0], [6, 3], [8, 2], [8, 0]]\n",
    "    points = np.array(points)\n",
    "\n",
    "    x = points[:,0]\n",
    "    y = points[:,1]\n",
    "\n",
    "\n",
    "    t = range(len(points))\n",
    "    ipl_t = np.linspace(0.0, len(points) - 1, 100)\n",
    "\n",
    "    x_tup = si.splrep(t, x, k=3)\n",
    "    y_tup = si.splrep(t, y, k=3)\n",
    "\n",
    "    x_list = list(x_tup)\n",
    "    xl = x.tolist()\n",
    "    x_list[1] = xl + [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "    y_list = list(y_tup)\n",
    "    yl = y.tolist()\n",
    "    y_list[1] = yl + [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "    x_i = si.splev(ipl_t, x_list) # x interpolate values\n",
    "    y_i = si.splev(ipl_t, y_list) # y interpolate values\n",
    "\n",
    "    \n",
    "    start_element = startElem\n",
    "\n",
    "    action.move_to_element(start_element)\n",
    "    action.perform()\n",
    "\n",
    "    c = 5\n",
    "    i = 0\n",
    "    for mouse_x, mouse_y in zip(x_i, y_i):\n",
    "        action.move_by_offset(mouse_x,mouse_y)\n",
    "        action.perform()\n",
    "        #print(\"Move mouse to, %s ,%s\" % (mouse_x, mouse_y))   \n",
    "        i += 1    \n",
    "        if i == c:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bypassCaptcha1():\n",
    "\n",
    "    # Switch to captcha iframe\n",
    "    iframe = browser.find_elements(By.TAG_NAME,'iframe')[0]\n",
    "    browser.switch_to.frame(iframe)\n",
    "    iframe = browser.find_elements(By.TAG_NAME,'iframe')[0]\n",
    "    browser.switch_to.frame(iframe)\n",
    "    \n",
    "    # Captcha check button\n",
    "    captcha_check = WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.XPATH, \"//*[@id='recaptcha-anchor']\")))\n",
    "    browser.implicitly_wait(12)\n",
    "    action =  ActionChains(browser)\n",
    "    # Simulate human like mouse movements\n",
    "    human_like_mouse(action, captcha_check)\n",
    "    captcha_check.click() # Click captcha check\n",
    "\n",
    "\n",
    "    browser.implicitly_wait(8)\n",
    "    action =  ActionChains(browser)\n",
    "    human_like_mouse(action, captcha_check)\n",
    "\n",
    "    browser.switch_to.default_content()\n",
    "\n",
    "    print('\\nSuccessfuly bypassed Captcha 1!\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if we get second captcha check\n",
    "#  if browser.find_elements(By.TAG_NAME,'iframe')[0].text.startswith('Request unsuccessful.'):\n",
    "\n",
    "def bypassCaptcha2():\n",
    "\n",
    "    t = random.randint(8,10)\n",
    "    browser.implicitly_wait(t)\n",
    "\n",
    "    iframe2 = browser.find_elements(By.TAG_NAME,\"iframe\")[0]\n",
    "    browser.switch_to.frame(iframe2)\n",
    "    iframe3 = browser.find_elements(By.XPATH,\"//iframe[@title='recaptcha challenge expires in two minutes']\")[0]\n",
    "    browser.switch_to.frame(iframe3)\n",
    "    # Find nearest element to #shadow-root (closed)\n",
    "    elem = browser.find_element(By.XPATH, '//div[@class=\"button-holder help-button-holder\"]')\n",
    "    browser.implicitly_wait(10)\n",
    "    elem.click() # click on it\n",
    "    browser.switch_to.default_content()\n",
    "\n",
    "    print('\\nSuccessfuly bypassed Captcha 2!\\n')  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping by keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'workday'\n",
    "\n",
    "base = 'https://www.higheredjobs.com/search/advanced_action.cfm?Remote=&Keyword='\n",
    "page_num = 1\n",
    "page = '&PosType=&InstType=&JobCat=&Region=0&SubRegions=&Metros=&OnlyTitle=0&JobCatType=&StartRow='+str(page_num)+'&SortBy=1&NumJobs=100&CatType='\n",
    "\n",
    "url = base+category+page"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape page links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workday\n"
     ]
    }
   ],
   "source": [
    "print(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 54 pages\n"
     ]
    }
   ],
   "source": [
    "# Scrape page links\n",
    "\n",
    "browser = dc.initialize_driver()\n",
    "t = random.randint(8,10)\n",
    "WebDriverWait(browser, t)\n",
    "browser.get(url)\n",
    "try:\n",
    "    # wait for page to load\n",
    "    WebDriverWait(browser, 20).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'text-nowrap'))\n",
    "    )\n",
    "    max_jobs = browser.find_element(By.CLASS_NAME, 'text-nowrap').text.split(' ')[5]\n",
    "    max_jobs = int(\"\".join(max_jobs.split(','))) # remove ',' so that we can convert to int \n",
    "    num_pages = math.ceil(max_jobs/100) # /100 because we iterate 100 links per page\n",
    "    browser.quit()\n",
    "\n",
    "    page_links = []\n",
    "    page_links.append(url) #1st page\n",
    "\n",
    "    # for page 2 until the last page\n",
    "    for p in range(1, num_pages): #=> we want to +100 for as many pages as we iterate through to the page link, starting at the 2nd page\n",
    "        page_num = page_num + 100\n",
    "        page = '&PosType=&InstType=&JobCat=&Region=0&SubRegions=&Metros=&OnlyTitle=0&JobCatType=&StartRow='+str(page_num)+'&SortBy=1&NumJobs=100&CatType='\n",
    "        url = base+category+page\n",
    "        page_links.append(url)\n",
    "\n",
    "    print(\"There are {} pages\".format(len(page_links)))\n",
    "    # Save page links\n",
    "    with open('Page_Links_'+category+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(page_links, f)\n",
    "\n",
    "except Exception: \n",
    "    # Check that exception is b/c of Captcha\n",
    "    if browser.find_elements(By.TAG_NAME,'iframe')[0].text.startswith('Request unsuccessful.'):\n",
    "        ppp = 0\n",
    "        while ppp < 1:\n",
    "            try:\n",
    "                WebDriverWait(browser, 60)\n",
    "                iframe = browser.find_elements(By.TAG_NAME,'iframe')[0]\n",
    "                WebDriverWait(browser, 10)\n",
    "                browser.switch_to.frame(iframe)\n",
    "                WebDriverWait(browser, 10)\n",
    "                browser.find_element(By.XPATH, \"//*[@id='recaptcha-anchor']\").click()\n",
    "                time.sleep(20) # manually solve image challenge => that worked\n",
    "                ppp = 1\n",
    "            except Exception:\n",
    "                ppp = 0\n",
    "        max_jobs = browser.find_element(By.CLASS_NAME, 'text-nowrap').text.split(' ')[5]\n",
    "        max_jobs = int(\"\".join(max_jobs.split(','))) # remove ',' so that we can convert to int \n",
    "        num_pages = math.ceil(max_jobs/100) # /100 because we iterate 100 links per page\n",
    "        browser.quit()\n",
    "\n",
    "        page_links = []\n",
    "        page_links.append(url) #1st page\n",
    "\n",
    "        # for page 2 until the last page\n",
    "        for p in range(1, num_pages): #=> we want to +100 for as many pages as we iterate through to the page link, starting at the 2nd page\n",
    "            page_num = page_num + 100\n",
    "            page = '&PosType=&InstType=&JobCat=&Region=0&SubRegions=&Metros=&OnlyTitle=0&JobCatType=&StartRow='+str(page_num)+'&SortBy=1&NumJobs=100&CatType='\n",
    "            url = base+x+page\n",
    "            page_links.append(url)\n",
    "\n",
    "        print(\"There are {} pages\".format(len(page_links)))\n",
    "        # Save page links\n",
    "        with open('Page_Links_'+category+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(page_links, f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2323, 54)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#job_links = []\n",
    "with open('Job_links_'+category+'.pkl', 'rb') as f:\n",
    "    job_links = pickle.load(f)\n",
    "\n",
    "with open('Page_Links_'+category+'.pkl', 'rb') as f:\n",
    "    page_links = pickle.load(f)\n",
    "\n",
    "job_attrs = []\n",
    "not_scraped = 0\n",
    "scraped = 0\n",
    "smile = 0\n",
    "\n",
    "len(job_links), len(page_links) # => page_links[24:], we are on page 23, but let's skip it and go on the next to avoid duplicates, we miss 67 jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing for 30\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Pausing 20 minutes, buster API overused!\n",
      "Trying again\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 1!\n",
      "\n",
      "\n",
      "Successfuly bypassed Captcha 2!\n",
      "\n",
      "Looking for job link\n",
      "\n",
      "Job_links:  5254\n",
      "There are 5201 jobs to scrape\n",
      "Job_links:  5201\n",
      "\n",
      "Scraping away...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 14\n",
      "\n",
      "\n",
      "Not scraped is 0\n",
      "\n",
      "Job 14 has been deleted\n",
      "Looking for job info 36\n",
      "\n",
      "\n",
      "Not scraped is 1\n",
      "\n",
      "Job 36 has been deleted\n",
      "Looking for job info 37\n",
      "\n",
      "\n",
      "Not scraped is 2\n",
      "\n",
      "Job 37 has been deleted\n",
      "Looking for job info 38\n",
      "\n",
      "\n",
      "Not scraped is 3\n",
      "\n",
      "Job 38 has been deleted\n",
      "Looking for job info 52\n",
      "\n",
      "\n",
      "Not scraped is 4\n",
      "\n",
      "Job 52 has been deleted\n",
      "Looking for job info 79\n",
      "\n",
      "\n",
      "Not scraped is 5\n",
      "\n",
      "Job 79 has been deleted\n",
      "94 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "\n",
      "Job 120 has been deleted\n",
      "193 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 203\n",
      "\n",
      "\n",
      "Not scraped is 7\n",
      "\n",
      "Job 203 has been deleted\n",
      "Looking for job info 216\n",
      "\n",
      "\n",
      "Not scraped is 8\n",
      "\n",
      "Job 216 has been deleted\n",
      "Looking for job info 277\n",
      "\n",
      "\n",
      "Not scraped is 9\n",
      "\n",
      "Job 277 has been deleted\n",
      "Looking for job info 285\n",
      "\n",
      "\n",
      "Not scraped is 10\n",
      "\n",
      "Job 285 has been deleted\n",
      "289 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 312\n",
      "\n",
      "\n",
      "Not scraped is 11\n",
      "\n",
      "Job 312 has been deleted\n",
      "Looking for job info 321\n",
      "\n",
      "\n",
      "Not scraped is 12\n",
      "\n",
      "Job 321 has been deleted\n",
      "Looking for job info 322\n",
      "\n",
      "\n",
      "Not scraped is 13\n",
      "\n",
      "Job 322 has been deleted\n",
      "Looking for job info 342\n",
      "\n",
      "\n",
      "Not scraped is 14\n",
      "\n",
      "Job 342 has been deleted\n",
      "Looking for job info 352\n",
      "\n",
      "\n",
      "Not scraped is 15\n",
      "\n",
      "Job 352 has been deleted\n",
      "Looking for job info 366\n",
      "\n",
      "\n",
      "Not scraped is 16\n",
      "\n",
      "Job 366 has been deleted\n",
      "Looking for job info 371\n",
      "\n",
      "\n",
      "Not scraped is 17\n",
      "\n",
      "Job 371 has been deleted\n",
      "Looking for job info 372\n",
      "\n",
      "\n",
      "Not scraped is 18\n",
      "\n",
      "Job 372 has been deleted\n",
      "Looking for job info 373\n",
      "\n",
      "\n",
      "Not scraped is 19\n",
      "\n",
      "Job 373 has been deleted\n",
      "380 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 407\n",
      "\n",
      "\n",
      "Not scraped is 20\n",
      "\n",
      "Job 407 has been deleted\n",
      "Looking for job info 465\n",
      "\n",
      "\n",
      "Not scraped is 21\n",
      "\n",
      "Job 465 has been deleted\n",
      "478 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 502\n",
      "\n",
      "\n",
      "Not scraped is 22\n",
      "\n",
      "Job 502 has been deleted\n",
      "Looking for job info 562\n",
      "\n",
      "\n",
      "Not scraped is 23\n",
      "\n",
      "Job 562 has been deleted\n",
      "576 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 665\n",
      "\n",
      "\n",
      "Not scraped is 24\n",
      "\n",
      "Job 665 has been deleted\n",
      "Looking for job info 683\n",
      "\n",
      "\n",
      "Not scraped is 25\n",
      "\n",
      "Job 683 has been deleted\n",
      "674 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 724\n",
      "\n",
      "\n",
      "Not scraped is 26\n",
      "\n",
      "Job 724 has been deleted\n",
      "Looking for job info 725\n",
      "\n",
      "\n",
      "Not scraped is 27\n",
      "\n",
      "Job 725 has been deleted\n",
      "Looking for job info 727\n",
      "\n",
      "\n",
      "Not scraped is 28\n",
      "\n",
      "Job 727 has been deleted\n",
      "Looking for job info 734\n",
      "\n",
      "\n",
      "Not scraped is 29\n",
      "\n",
      "Job 734 has been deleted\n",
      "Looking for job info 772\n",
      "\n",
      "\n",
      "Not scraped is 30\n",
      "\n",
      "Job 772 has been deleted\n",
      "769 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 807\n",
      "\n",
      "\n",
      "Not scraped is 31\n",
      "\n",
      "Job 807 has been deleted\n",
      "Looking for job info 815\n",
      "\n",
      "\n",
      "Not scraped is 32\n",
      "\n",
      "Job 815 has been deleted\n",
      "Looking for job info 817\n",
      "\n",
      "\n",
      "Not scraped is 33\n",
      "\n",
      "Job 817 has been deleted\n",
      "Looking for job info 824\n",
      "\n",
      "\n",
      "Not scraped is 34\n",
      "\n",
      "Job 824 has been deleted\n",
      "Looking for job info 858\n",
      "\n",
      "\n",
      "Not scraped is 35\n",
      "\n",
      "Job 858 has been deleted\n",
      "Looking for job info 859\n",
      "\n",
      "\n",
      "Not scraped is 36\n",
      "\n",
      "Job 859 has been deleted\n",
      "Looking for job info 882\n",
      "\n",
      "\n",
      "Not scraped is 37\n",
      "\n",
      "Job 882 has been deleted\n",
      "862 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 914\n",
      "\n",
      "\n",
      "Not scraped is 38\n",
      "\n",
      "Job 914 has been deleted\n",
      "961 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 1029\n",
      "\n",
      "\n",
      "Not scraped is 39\n",
      "\n",
      "Job 1029 has been deleted\n",
      "Looking for job info 1087\n",
      "\n",
      "\n",
      "Not scraped is 40\n",
      "\n",
      "Job 1087 has been deleted\n",
      "1059 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 1171\n",
      "\n",
      "\n",
      "Not scraped is 41\n",
      "\n",
      "Job 1171 has been deleted\n",
      "1158 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 1278\n",
      "\n",
      "\n",
      "Not scraped is 42\n",
      "\n",
      "Job 1278 has been deleted\n",
      "1257 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 1307\n",
      "\n",
      "\n",
      "Not scraped is 43\n",
      "\n",
      "Job 1307 has been deleted\n",
      "1356 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "1456 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "1556 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 1665\n",
      "\n",
      "\n",
      "Not scraped is 44\n",
      "\n",
      "Job 1665 has been deleted\n",
      "1655 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 1705\n",
      "\n",
      "\n",
      "Not scraped is 45\n",
      "\n",
      "Job 1705 has been deleted\n",
      "Looking for job info 1731\n",
      "\n",
      "\n",
      "Not scraped is 46\n",
      "\n",
      "Job 1731 has been deleted\n",
      "Looking for job info 1774\n",
      "\n",
      "\n",
      "Not scraped is 47\n",
      "\n",
      "Job 1774 has been deleted\n",
      "\n",
      "Job 1787 has been deleted\n",
      "1751 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 1813\n",
      "\n",
      "\n",
      "Not scraped is 49\n",
      "\n",
      "Job 1813 has been deleted\n",
      "\n",
      "Job 1883 has been deleted\n",
      "1849 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 1913\n",
      "\n",
      "\n",
      "Not scraped is 51\n",
      "\n",
      "Job 1913 has been deleted\n",
      "Looking for job info 1916\n",
      "\n",
      "\n",
      "Not scraped is 52\n",
      "\n",
      "Job 1916 has been deleted\n",
      "Looking for job info 1972\n",
      "\n",
      "\n",
      "Not scraped is 53\n",
      "\n",
      "Job 1972 has been deleted\n",
      "Looking for job info 1973\n",
      "\n",
      "\n",
      "Not scraped is 54\n",
      "\n",
      "Job 1973 has been deleted\n",
      "Looking for job info 1980\n",
      "\n",
      "\n",
      "Not scraped is 55\n",
      "\n",
      "Job 1980 has been deleted\n",
      "1944 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 2014\n",
      "\n",
      "\n",
      "Not scraped is 56\n",
      "\n",
      "Job 2014 has been deleted\n",
      "\n",
      "Job 2016 has been deleted\n",
      "Looking for job info 2020\n",
      "\n",
      "\n",
      "Not scraped is 58\n",
      "\n",
      "Job 2020 has been deleted\n",
      "Looking for job info 2034\n",
      "\n",
      "\n",
      "Not scraped is 59\n",
      "\n",
      "Job 2034 has been deleted\n",
      "Looking for job info 2035\n",
      "\n",
      "\n",
      "Not scraped is 60\n",
      "\n",
      "Job 2035 has been deleted\n",
      "Looking for job info 2045\n",
      "\n",
      "\n",
      "Not scraped is 61\n",
      "\n",
      "Job 2045 has been deleted\n",
      "Looking for job info 2048\n",
      "\n",
      "\n",
      "Not scraped is 62\n",
      "\n",
      "Job 2048 has been deleted\n",
      "Looking for job info 2054\n",
      "\n",
      "\n",
      "Not scraped is 63\n",
      "\n",
      "Job 2054 has been deleted\n",
      "Looking for job info 2058\n",
      "\n",
      "\n",
      "Not scraped is 64\n",
      "\n",
      "Job 2058 has been deleted\n",
      "Looking for job info 2064\n",
      "\n",
      "\n",
      "Not scraped is 65\n",
      "\n",
      "Job 2064 has been deleted\n",
      "2034 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 2181\n",
      "\n",
      "\n",
      "Not scraped is 66\n",
      "\n",
      "Job 2181 has been deleted\n",
      "\n",
      "Job 2187 has been deleted\n",
      "Looking for job info 2196\n",
      "\n",
      "\n",
      "Not scraped is 68\n",
      "\n",
      "Job 2196 has been deleted\n",
      "2131 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 2233\n",
      "\n",
      "\n",
      "Not scraped is 69\n",
      "\n",
      "Job 2233 has been deleted\n",
      "Looking for job info 2239\n",
      "\n",
      "\n",
      "Not scraped is 70\n",
      "\n",
      "Job 2239 has been deleted\n",
      "Looking for job info 2240\n",
      "\n",
      "\n",
      "Not scraped is 71\n",
      "\n",
      "Job 2240 has been deleted\n",
      "Looking for job info 2241\n",
      "\n",
      "\n",
      "Not scraped is 72\n",
      "\n",
      "Job 2241 has been deleted\n",
      "Looking for job info 2248\n",
      "\n",
      "\n",
      "Not scraped is 73\n",
      "\n",
      "Job 2248 has been deleted\n",
      "Looking for job info 2273\n",
      "\n",
      "\n",
      "Not scraped is 74\n",
      "\n",
      "Job 2273 has been deleted\n",
      "Looking for job info 2276\n",
      "\n",
      "\n",
      "Not scraped is 75\n",
      "\n",
      "Job 2276 has been deleted\n",
      "Looking for job info 2284\n",
      "\n",
      "\n",
      "Not scraped is 76\n",
      "\n",
      "Job 2284 has been deleted\n",
      "Looking for job info 2287\n",
      "\n",
      "\n",
      "Not scraped is 77\n",
      "\n",
      "Job 2287 has been deleted\n",
      "2222 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 2358\n",
      "\n",
      "\n",
      "Not scraped is 78\n",
      "\n",
      "Job 2358 has been deleted\n",
      "2321 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 2405\n",
      "\n",
      "\n",
      "Not scraped is 79\n",
      "\n",
      "Job 2405 has been deleted\n",
      "Looking for job info 2412\n",
      "\n",
      "\n",
      "Not scraped is 80\n",
      "\n",
      "Job 2412 has been deleted\n",
      "2419 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 2543\n",
      "\n",
      "\n",
      "Not scraped is 81\n",
      "\n",
      "Job 2543 has been deleted\n",
      "2518 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 2602\n",
      "\n",
      "\n",
      "Not scraped is 82\n",
      "\n",
      "Job 2602 has been deleted\n",
      "2617 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 2723\n",
      "\n",
      "\n",
      "Not scraped is 83\n",
      "\n",
      "Job 2723 has been deleted\n",
      "2716 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 2828\n",
      "\n",
      "\n",
      "Not scraped is 84\n",
      "\n",
      "Job 2828 has been deleted\n",
      "2815 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 2949\n",
      "\n",
      "\n",
      "Not scraped is 85\n",
      "\n",
      "Job 2949 has been deleted\n",
      "2914 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "3014 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 3145\n",
      "\n",
      "\n",
      "Not scraped is 86\n",
      "\n",
      "Job 3145 has been deleted\n",
      "Looking for job info 3179\n",
      "\n",
      "\n",
      "Not scraped is 87\n",
      "\n",
      "Job 3179 has been deleted\n",
      "3112 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "3212 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "3312 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 3418\n",
      "\n",
      "\n",
      "Not scraped is 88\n",
      "\n",
      "Job 3418 has been deleted\n",
      "Looking for job info 3452\n",
      "\n",
      "\n",
      "Not scraped is 89\n",
      "\n",
      "Job 3452 has been deleted\n",
      "Looking for job info 3454\n",
      "\n",
      "\n",
      "Not scraped is 90\n",
      "\n",
      "Job 3454 has been deleted\n",
      "Looking for job info 3498\n",
      "\n",
      "\n",
      "Not scraped is 91\n",
      "\n",
      "Job 3498 has been deleted\n",
      "3408 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 3509\n",
      "\n",
      "\n",
      "Not scraped is 92\n",
      "\n",
      "Job 3509 has been deleted\n",
      "Looking for job info 3572\n",
      "\n",
      "\n",
      "Not scraped is 93\n",
      "\n",
      "Job 3572 has been deleted\n",
      "Looking for job info 3581\n",
      "\n",
      "\n",
      "Not scraped is 94\n",
      "\n",
      "Job 3581 has been deleted\n",
      "3505 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 3668\n",
      "\n",
      "\n",
      "Not scraped is 95\n",
      "\n",
      "Job 3668 has been deleted\n",
      "3604 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "3704 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 3855\n",
      "\n",
      "\n",
      "Not scraped is 96\n",
      "\n",
      "Job 3855 has been deleted\n",
      "Looking for job info 3860\n",
      "\n",
      "\n",
      "Not scraped is 97\n",
      "\n",
      "Job 3860 has been deleted\n",
      "3802 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "3902 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "4002 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16080\\3711981647.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mjob_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'jobtitle-header'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             \u001b[0mjob_loc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLASS_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'job-loc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ckelaid\\Anaconda3\\envs\\scrape\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFIND_ELEMENT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"using\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"value\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ckelaid\\Anaconda3\\envs\\scrape\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    443\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ckelaid\\Anaconda3\\envs\\scrape\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"jobtitle-header\"]\"}\n  (Session info: chrome=111.0.5563.147)\nStacktrace:\nBacktrace:\n\t(No symbol) [0x00BADCE3]\n\t(No symbol) [0x00B439D1]\n\t(No symbol) [0x00A54DA8]\n\t(No symbol) [0x00A8019F]\n\t(No symbol) [0x00A803AB]\n\t(No symbol) [0x00AAEE62]\n\t(No symbol) [0x00A9AF14]\n\t(No symbol) [0x00AAD57C]\n\t(No symbol) [0x00A9ACC6]\n\t(No symbol) [0x00A76F68]\n\t(No symbol) [0x00A780CD]\n\tGetHandleVerifier [0x00E23832+2506274]\n\tGetHandleVerifier [0x00E59794+2727300]\n\tGetHandleVerifier [0x00E5E36C+2746716]\n\tGetHandleVerifier [0x00C56690+617600]\n\t(No symbol) [0x00B4C712]\n\t(No symbol) [0x00B51FF8]\n\t(No symbol) [0x00B520DB]\n\t(No symbol) [0x00B5C63B]\n\tBaseThreadInitThunk [0x76A700F9+25]\n\tRtlGetAppContainerNamedObjectPath [0x77CA7BBE+286]\n\tRtlGetAppContainerNamedObjectPath [0x77CA7B8E+238]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16080\\3711981647.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;31m# See if exception is related to captcha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTAG_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'iframe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Request unsuccessful.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[1;31m#bypassFullCaptcha()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Scraping job links + cleaning job links + scraping job attributes\n",
    "\n",
    "for link in page_links[24:]:\n",
    "    t = random.randint(8,10)\n",
    "    time.sleep(t)\n",
    "    browser = dc.initialize_driver()\n",
    "    browser.get(link)\n",
    "    try:\n",
    "        # wait for page to load\n",
    "        WebDriverWait(browser, 20).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'col-sm-7'))\n",
    "        )\n",
    "        # get job links\n",
    "        div = browser.find_elements(By.CLASS_NAME, 'col-sm-7')\n",
    "        for d in div:\n",
    "            job_links.append(d.find_element(By.CSS_SELECTOR, 'a').get_attribute('href'))\n",
    "        browser.quit()\n",
    "        with open('Job_links_'+category+'.pkl', 'wb') as f:\n",
    "            pickle.dump(job_links, f)\n",
    "    except Exception:\n",
    "        # print('bypassing Captcha')\n",
    "        # WebDriverWait(browser, 10).until(\n",
    "        #     EC.presence_of_all_elements_located((By.TAG_NAME,'iframe'))\n",
    "        # )\n",
    "        # Check that exception is b/c of Captcha\n",
    "        if browser.find_elements(By.TAG_NAME,'iframe')[0].text.startswith('Request unsuccessful.'):\n",
    "\n",
    "            #bypassFullCaptcha()\n",
    "            bypassCaptcha1()\n",
    "            \n",
    "            # Check if we get second captcha check\n",
    "            if browser.find_elements(By.TAG_NAME,'iframe')[0].text.startswith('Request unsuccessful.'):\n",
    "                \n",
    "                bypassCaptcha2()\n",
    "                time.sleep(10)\n",
    "\n",
    "        try:\n",
    "            print('Looking for job link\\n')\n",
    "                \n",
    "            # Successfully moved out of Captcha\n",
    "\n",
    "            # wait for page to load\n",
    "            WebDriverWait(browser, 20).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, 'col-sm-7'))\n",
    "            ) \n",
    "            # get job links\n",
    "            div = browser.find_elements(By.CLASS_NAME, 'col-sm-7')\n",
    "            for d in div:\n",
    "                job_links.append(d.find_element(By.CSS_SELECTOR, 'a').get_attribute('href'))\n",
    "            browser.quit()\n",
    "            with open('Job_links_'+category+'.pkl', 'wb') as f:\n",
    "                pickle.dump(job_links, f)\n",
    "        \n",
    "        except Exception:\n",
    "            # API limit exceeded and need to wait, pause 20 minutes and try again\n",
    "            browser.quit()\n",
    "            print('Pausing 20 minutes, buster API overused!')\n",
    "            time.sleep(1200) # wait 20 minutes :( looooooong\n",
    "\n",
    "            pp = 0\n",
    "            aa = 0\n",
    "            while pp < 1:\n",
    "                try:\n",
    "                    browser = dc.initialize_driver()\n",
    "                    browser.get(link)\n",
    "\n",
    "                    #browser.switch_to.default_content()\n",
    "                    \n",
    "                    if browser.find_elements(By.TAG_NAME,'iframe')[0].text.startswith('Request unsuccessful.'):\n",
    "                        print('Trying again')\n",
    "\n",
    "                        time.sleep(10)\n",
    "\n",
    "                        bypassCaptcha1()\n",
    "\n",
    "                        # Check if we get second captcha check\n",
    "                        if browser.find_elements(By.TAG_NAME,'iframe')[0].text.startswith('Request unsuccessful.'):\n",
    "\n",
    "                            bypassCaptcha2()\n",
    "                            time.sleep(10)\n",
    "\n",
    "                        print('Looking for job link\\n')\n",
    "                            \n",
    "                        # Successfully moved out of Captcha\n",
    "\n",
    "                        # wait for page to load\n",
    "                        WebDriverWait(browser, 20).until(\n",
    "                                EC.presence_of_element_located((By.CLASS_NAME, 'col-sm-7'))\n",
    "                        ) \n",
    "                        # get job links\n",
    "                        div = browser.find_elements(By.CLASS_NAME, 'col-sm-7')\n",
    "                        for d in div:\n",
    "                            job_links.append(d.find_element(By.CSS_SELECTOR, 'a').get_attribute('href'))\n",
    "                        browser.quit()\n",
    "                        with open('Job_links_'+category+'.pkl', 'wb') as f:\n",
    "                            pickle.dump(job_links, f)\n",
    "                        pp = 1\n",
    "                    \n",
    "                except Exception:\n",
    "                    time.sleep(60)\n",
    "                    browser.implicitly_wait(7)\n",
    "                    browser.quit()\n",
    "                    pp = 0\n",
    "                    aa = aa+1\n",
    "                    if aa == 5:\n",
    "                        print('Pausing for 30')\n",
    "                        time.sleep(1800) # 30 minutes\n",
    "\n",
    "                    if aa == 6:\n",
    "                        print('Pausing for 3 hours!')\n",
    "                        time.sleep(10800)\n",
    "                    \n",
    "                    if aa > 6:\n",
    "                        print('Pause for 2 min intervals until it works')\n",
    "                        time.sleep(120)\n",
    "                    \n",
    "                    if aa == 12:\n",
    "                        print('Pause for 3 hours')\n",
    "                        time.sleep(10800)\n",
    "\n",
    "# Done with job links\n",
    "print(\"Job_links: \",len(job_links))\n",
    "\n",
    "######################################\n",
    "\n",
    "# Cleaning job_links\n",
    "\n",
    "######################################\n",
    "\n",
    "# Deleting wrong entries\n",
    "del_ix = []\n",
    "for i in range(len(job_links)):\n",
    "    if job_links[i] == 'javascript:;':\n",
    "        del_ix.append(i) \n",
    "\n",
    "# Also check that no job_links == category_page url\n",
    "for j in range(len(job_links)):\n",
    "    if job_links[j] == 'https://www.higheredjobs.com/search/advanced_action.cfm?Remote=&Keyword='+category+'&PosType=&InstType=&JobCat=&Region=0&SubRegions=&Metros=&OnlyTitle=0&JobCatType=&SortBy=1&StartRow=1':\n",
    "        del_ix.append(j)\n",
    "\n",
    "if len(del_ix) > 0:\n",
    "    # delete 'javascript:;' entries & incorrect url entries\n",
    "    del job_links[del_ix[0]]\n",
    "\n",
    "    drop = 1\n",
    "    for i in range(1, len(del_ix)):\n",
    "        del job_links[del_ix[i]-drop]\n",
    "        drop = drop + 1\n",
    "\n",
    "print(\"There are {} jobs to scrape\".format(len(job_links)))\n",
    "\n",
    "# Save job links\n",
    "with open('Job_links_'+category+'.pkl', 'wb') as f:\n",
    "            pickle.dump(job_links, f)\n",
    "\n",
    "\n",
    "# Done cleaning job links\n",
    "\n",
    "print(\"Job_links: \",len(job_links))\n",
    "\n",
    "######################################\n",
    "\n",
    "\n",
    "####### Scraping job attributes ######\n",
    "\n",
    "\n",
    "######################################\n",
    "\n",
    "# job_attrs = []\n",
    "# not_scraped = 0\n",
    "# scraped = 0\n",
    "# smile = 0\n",
    "\n",
    "\n",
    "for link in job_links: #start at job 212, bc job 1 is index 0\n",
    "    t = random.randint(8,10)\n",
    "    time.sleep(t)\n",
    "    browser = dc.initialize_driver()\n",
    "    browser.get(link)\n",
    "\n",
    "    try:\n",
    "        # wait for page to load\n",
    "        WebDriverWait(browser, 20).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'row'))\n",
    "        )\n",
    "        del_div = browser.find_elements(By.CLASS_NAME, 'row')\n",
    "        # Check if post has been deleted\n",
    "        if del_div[6].text != 'We require users to verify the reCaptcha below to view deleted positions.\\nRelated Searches:\\nBusiness and Financial Services\\nCreate your free job search account\\nReceive new jobs by email\\nPost your resume/CV\\nTrack your applications\\nJoin Now\\nHave an account? Sign in':\n",
    "\n",
    "            job_title = browser.find_element(By.ID, 'jobtitle-header').text\n",
    "            job_loc = browser.find_element(By.CLASS_NAME, 'job-loc').text\n",
    "            div = browser.find_elements(By.ID, 'jobAttrib')\n",
    "            job_attr = div[0].find_element(By.CLASS_NAME, 'job-info').text.split('\\n') # get job data\n",
    "       \n",
    "            div = browser.find_elements(By.ID, 'job')\n",
    "            job_desc = div[0].find_element(By.ID, 'jobDesc').text#.split('\\n')\n",
    "            \n",
    "            job_attr = [job_title] + [job_loc] + job_attr # puts title at the begining of the list\n",
    "            job_attr.append(job_desc) # Append job_description\n",
    "            job_attrs.append(job_attr) # Append job details as list inside another list \n",
    "            \n",
    "            scraped = scraped + 1\n",
    "\n",
    "            browser.quit()\n",
    "            smile = smile +1\n",
    "            if smile == 1:\n",
    "                print('\\nScraping away...\\n')\n",
    "                print('0  0\\n')\n",
    "                print('\\__/\\n')\n",
    "            if smile % 100 == 0:\n",
    "                print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                print('0  0\\n')\n",
    "                print('\\__/\\n')\n",
    "            \n",
    "            # Save progress (every job info)\n",
    "            #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                # Store data (serialize)\n",
    "            with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                pickle.dump(job_attrs, f)\n",
    "\n",
    "        elif IndexError:\n",
    "            print('\\nJob {} has been deleted'.format(smile))\n",
    "            not_scraped = not_scraped + 1\n",
    "\n",
    "            browser.quit()\n",
    "            smile = smile +1\n",
    "            if smile == 1:\n",
    "                print('\\nScraping away...\\n')\n",
    "                print('0  0\\n')\n",
    "                print('\\__/\\n')\n",
    "            if smile % 100 == 0:\n",
    "                print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                print('0  0\\n')\n",
    "                print('\\__/\\n')\n",
    "            \n",
    "            # Save progress (every job info)\n",
    "            #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                # Store data (serialize)\n",
    "            with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                pickle.dump(job_attrs, f)\n",
    "\n",
    "    except Exception: # If cannot find element\n",
    "\n",
    "        # # wait for page to load\n",
    "        # WebDriverWait(browser, 20).until(\n",
    "        #     EC.presence_of_element_located((By.CLASS_NAME, 'iframe'))\n",
    "        # ) \n",
    "               \n",
    "        # See if exception is related to captcha\n",
    "        if browser.find_elements(By.TAG_NAME,'iframe')[0].text.startswith('Request unsuccessful.'):\n",
    "            \n",
    "            #bypassFullCaptcha()\n",
    "            bypassCaptcha1()\n",
    "            \n",
    "            # Check if we get second captcha check\n",
    "            if browser.find_elements(By.TAG_NAME,'iframe')[0].text.startswith('Request unsuccessful.'):\n",
    "                \n",
    "                bypassCaptcha2()\n",
    "                time.sleep(10)\n",
    "        \n",
    "        try:\n",
    "            print('Looking for job info {}\\n'.format(smile))\n",
    "            print('\\nNot scraped is', not_scraped)\n",
    "            # Successfully moved out of Captcha\n",
    "\n",
    "            # wait for page to load\n",
    "            WebDriverWait(browser, 20).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'row'))\n",
    "            )\n",
    "            del_div = browser.find_elements(By.CLASS_NAME, 'row')\n",
    "            # Check if post has been deleted\n",
    "            if del_div[6].text != 'We require users to verify the reCaptcha below to view deleted positions.\\nRelated Searches:\\nBusiness and Financial Services\\nCreate your free job search account\\nReceive new jobs by email\\nPost your resume/CV\\nTrack your applications\\nJoin Now\\nHave an account? Sign in':\n",
    "\n",
    "                job_title = browser.find_element(By.ID, 'jobtitle-header').text\n",
    "                job_loc = browser.find_element(By.CLASS_NAME, 'job-loc').text\n",
    "                div = browser.find_elements(By.ID, 'jobAttrib')\n",
    "                job_attr = div[0].find_element(By.CLASS_NAME, 'job-info').text.split('\\n') # get job data\n",
    "                        #job_attr = [job_title] + job_attr\n",
    "                div = browser.find_elements(By.ID, 'job')\n",
    "                job_desc = div[0].find_element(By.ID, 'jobDesc').text#.split('\\n')\n",
    "                    #job_attr.append(job_desc)\n",
    "                    #job_attrs.append(job_attr)\n",
    "                \n",
    "            #else: # if post is delete\n",
    "                job_attr = [job_title] + [job_loc] + job_attr # puts title at the begining of the list\n",
    "                job_attr.append(job_desc) # Append job_description\n",
    "                job_attrs.append(job_attr) # Append job details as list inside another list \n",
    "                \n",
    "                scraped = scraped + 1\n",
    "                \n",
    "                browser.quit()\n",
    "                smile = smile +1\n",
    "                if smile == 1:\n",
    "                    print('\\nScraping away...\\n')\n",
    "                    print('0  0\\n')\n",
    "                    print('\\__/\\n')\n",
    "                if smile % 100 == 0:\n",
    "                    print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                    print('0  0\\n')\n",
    "                    print('\\__/\\n')\n",
    "                \n",
    "                # Save progress (every job info)\n",
    "                #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                    # Store data (serialize)\n",
    "                with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(job_attrs, f)\n",
    "\n",
    "            elif IndexError:\n",
    "                print('\\nJob {} has been deleted'.format(smile))\n",
    "                not_scraped = not_scraped + 1\n",
    "\n",
    "                browser.quit()\n",
    "                smile = smile +1\n",
    "                if smile == 1:\n",
    "                    print('\\nScraping away...\\n')\n",
    "                    print('0  0\\n')\n",
    "                    print('\\__/\\n')\n",
    "                if smile % 100 == 0:\n",
    "                    print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                    print('0  0\\n')\n",
    "                    print('\\__/\\n')\n",
    "                \n",
    "                # Save progress (every job info)\n",
    "                #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                    # Store data (serialize)\n",
    "                with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(job_attrs, f)\n",
    "            \n",
    "        except Exception:\n",
    "\n",
    "            try:\n",
    "                if del_div[6].text.startswith('We require users to verify the reCaptcha below to view deleted positions'):\n",
    "                    \n",
    "                    print('\\nJob {} has been deleted'.format(smile))\n",
    "                    not_scraped = not_scraped + 1\n",
    "\n",
    "                    browser.quit()\n",
    "                    smile = smile +1\n",
    "                    if smile == 1:\n",
    "                        print('\\nScraping away...\\n')\n",
    "                        print('0  0\\n')\n",
    "                        print('\\__/\\n')\n",
    "                    if smile % 100 == 0:\n",
    "                        print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                        print('0  0\\n')\n",
    "                        print('\\__/\\n')\n",
    "                    \n",
    "                    # Save progress (every job info)\n",
    "                    #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                        # Store data (serialize)\n",
    "                    with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                        pickle.dump(job_attrs, f)\n",
    "            \n",
    "            except Exception:\n",
    "            # If exception is not b/c job is deleted then we need to pause\n",
    "                # API limit exceeded and need to wait, pause 20 minutes and try again\n",
    "                browser.quit()\n",
    "                print('Pausing 20 minutes, buster API overused!')\n",
    "                time.sleep(1200) # wait 20 minutes :( looooooong\n",
    "\n",
    "                pp = 0\n",
    "                aa = 0\n",
    "                while pp < 1:\n",
    "                    try:\n",
    "                        browser = dc.initialize_driver()\n",
    "                        browser.get(link)\n",
    "\n",
    "                        #browser.switch_to.default_content()\n",
    "                        \n",
    "                        if browser.find_elements(By.TAG_NAME,'iframe')[0].text.startswith('Request unsuccessful.'):\n",
    "                            print('Trying again on job ', smile)\n",
    "\n",
    "                            time.sleep(10)\n",
    "\n",
    "                            bypassCaptcha1()\n",
    "\n",
    "                            bypassCaptcha2()\n",
    "                            time.sleep(10)\n",
    "\n",
    "                            print('Looking for job info {}\\n'.format(smile))\n",
    "                            print('\\nNot scraped is', not_scraped)\n",
    "                                \n",
    "                            # Successfully moved out of Captcha\n",
    "\n",
    "                            # wait for page to load\n",
    "                            WebDriverWait(browser, 20).until(\n",
    "                                EC.presence_of_element_located((By.CLASS_NAME, 'row'))\n",
    "                            )\n",
    "                            del_div = browser.find_elements(By.CLASS_NAME, 'row')\n",
    "                            # Check if post has been deleted\n",
    "                            if del_div[6].text != 'We require users to verify the reCaptcha below to view deleted positions.\\nRelated Searches:\\nBusiness and Financial Services\\nCreate your free job search account\\nReceive new jobs by email\\nPost your resume/CV\\nTrack your applications\\nJoin Now\\nHave an account? Sign in':\n",
    "\n",
    "                                job_title = browser.find_element(By.ID, 'jobtitle-header').text\n",
    "                                job_loc = browser.find_element(By.CLASS_NAME, 'job-loc').text\n",
    "                                div = browser.find_elements(By.ID, 'jobAttrib')\n",
    "                                job_attr = div[0].find_element(By.CLASS_NAME, 'job-info').text.split('\\n') # get job data\n",
    "                                        #job_attr = [job_title] + job_attr\n",
    "                                div = browser.find_elements(By.ID, 'job')\n",
    "                                job_desc = div[0].find_element(By.ID, 'jobDesc').text#.split('\\n')\n",
    "                                    #job_attr.append(job_desc)\n",
    "                                    #job_attrs.append(job_attr)\n",
    "                                \n",
    "                            #else: # if post is delete\n",
    "                                job_attr = [job_title] + [job_loc] + job_attr # puts title at the begining of the list\n",
    "                                job_attr.append(job_desc) # Append job_description\n",
    "                                job_attrs.append(job_attr) # Append job details as list inside another list \n",
    "                                \n",
    "                                scraped = scraped + 1\n",
    "                                \n",
    "                                browser.quit()\n",
    "                                smile = smile +1\n",
    "                                if smile == 1:\n",
    "                                    print('\\nScraping away...\\n')\n",
    "                                    print('0  0\\n')\n",
    "                                    print('\\__/\\n')\n",
    "                                if smile % 100 == 0:\n",
    "                                    print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                                    print('0  0\\n')\n",
    "                                    print('\\__/\\n')\n",
    "                                \n",
    "                                # Save progress (every job info)\n",
    "                                #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                                    # Store data (serialize)\n",
    "                                with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                                    pickle.dump(job_attrs, f)\n",
    "                                pp = 1\n",
    "\n",
    "                            elif IndexError:\n",
    "                                print('\\nJob {} has been deleted'.format(smile))\n",
    "                                not_scraped = not_scraped + 1\n",
    "\n",
    "                                browser.quit()\n",
    "                                smile = smile +1\n",
    "                                if smile == 1:\n",
    "                                    print('\\nScraping away...\\n')\n",
    "                                    print('0  0\\n')\n",
    "                                    print('\\__/\\n')\n",
    "                                if smile % 100 == 0:\n",
    "                                    print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                                    print('0  0\\n')\n",
    "                                    print('\\__/\\n')\n",
    "                                \n",
    "                                # Save progress (every job info)\n",
    "                                #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                                    # Store data (serialize)\n",
    "                                with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                                    pickle.dump(job_attrs, f)\n",
    "                                pp = 1\n",
    "                            \n",
    "                                \n",
    "                    except Exception:\n",
    "                        time.sleep(60)\n",
    "                        try:\n",
    "                            if del_div[6].text.startswith('We require users to verify the reCaptcha below to view deleted positions'):\n",
    "                    \n",
    "                                print('\\nJob {} has been deleted'.format(smile))\n",
    "                                not_scraped = not_scraped + 1\n",
    "\n",
    "                                browser.quit()\n",
    "                                smile = smile +1\n",
    "                                if smile == 1:\n",
    "                                    print('\\nScraping away...\\n')\n",
    "                                    print('0  0\\n')\n",
    "                                    print('\\__/\\n')\n",
    "                                if smile % 100 == 0:\n",
    "                                    print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                                    print('0  0\\n')\n",
    "                                    print('\\__/\\n')\n",
    "                                \n",
    "                                # Save progress (every job info)\n",
    "                                #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                                    # Store data (serialize)\n",
    "                                with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                                    pickle.dump(job_attrs, f)\n",
    "                                pp = 1\n",
    "                        \n",
    "                        except Exception:\n",
    "\n",
    "                            browser.implicitly_wait(7)\n",
    "                            browser.quit()\n",
    "                            pp = 0\n",
    "                            aa = aa + 1\n",
    "                            print('\\naa is \\n', aa)\n",
    "                            if aa == 5: #Let it try a few times\n",
    "                                print('Pausing for 30 minutes')\n",
    "                                time.sleep(1800) \n",
    "                            \n",
    "                            if aa == 6:\n",
    "                                print('Pausing for 3 hours!')\n",
    "                                time.sleep(10800)\n",
    "                            \n",
    "                            if aa > 6:\n",
    "                                print('Pause for 2 min intervals until it works')\n",
    "                                time.sleep(120)\n",
    "                            \n",
    "                            if aa == 12:\n",
    "                                print('Pause for 3 hours')\n",
    "                                time.sleep(10800)\n",
    "\n",
    "                    \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        # else: # if exception not related to captcha, than simply info doesn't exist\n",
    "        #     not_scraped = not_scraped + 1\n",
    "\n",
    "if not_scraped > 0:\n",
    "\n",
    "    if not_scraped == 1:\n",
    "        print('Unfortunately, information for {} job was unable to be scraped.'.format(not_scraped))\n",
    "    \n",
    "    elif not_scraped > 1:\n",
    "        print('Unfortunately, information for {} jobs was unable to be scraped.'.format(not_scraped))\n",
    "\n",
    "print(\"\\nInformation on a total of {} jobs has been scraped\\n\".format(scraped))\n",
    "\n",
    "print(\"\\nLenght of job_attrs is: {}\\n\".format(len(job_attrs)))\n",
    "\n",
    "# Done scrpaing job attributes\n",
    "\n",
    "##################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exception came up, so we checked to see if captcha was there, however not there ==> different never have had before exception\n",
    "Hence ==> NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4002 98 4100\n"
     ]
    }
   ],
   "source": [
    "print(scraped, not_scraped, smile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5201"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_links) # so missing just a few, Oh we're good"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rerunning on those we missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for job info 4106\n",
      "\n",
      "\n",
      "Not scraped is 98\n",
      "\n",
      "Job 4106 has been deleted\n",
      "Looking for job info 4143\n",
      "\n",
      "\n",
      "Not scraped is 99\n",
      "\n",
      "Job 4143 has been deleted\n",
      "4100 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 4212\n",
      "\n",
      "\n",
      "Not scraped is 100\n",
      "\n",
      "Job 4212 has been deleted\n",
      "Looking for job info 4267\n",
      "\n",
      "\n",
      "Not scraped is 101\n",
      "\n",
      "Job 4267 has been deleted\n",
      "4198 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "4298 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 4429\n",
      "\n",
      "\n",
      "Not scraped is 102\n",
      "\n",
      "Job 4429 has been deleted\n",
      "Looking for job info 4480\n",
      "\n",
      "\n",
      "Not scraped is 103\n",
      "\n",
      "Job 4480 has been deleted\n",
      "4396 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 4528\n",
      "\n",
      "\n",
      "Not scraped is 104\n",
      "\n",
      "Job 4528 has been deleted\n",
      "Looking for job info 4565\n",
      "\n",
      "\n",
      "Not scraped is 105\n",
      "\n",
      "Job 4565 has been deleted\n",
      "4494 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 4622\n",
      "\n",
      "\n",
      "Not scraped is 106\n",
      "\n",
      "Job 4622 has been deleted\n",
      "Looking for job info 4682\n",
      "\n",
      "\n",
      "Not scraped is 107\n",
      "\n",
      "Job 4682 has been deleted\n",
      "Looking for job info 4688\n",
      "\n",
      "\n",
      "Not scraped is 108\n",
      "\n",
      "Job 4688 has been deleted\n",
      "4591 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 4741\n",
      "\n",
      "\n",
      "Not scraped is 109\n",
      "\n",
      "Job 4741 has been deleted\n",
      "4690 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 4818\n",
      "\n",
      "\n",
      "Not scraped is 110\n",
      "\n",
      "Job 4818 has been deleted\n",
      "Looking for job info 4835\n",
      "\n",
      "\n",
      "Not scraped is 111\n",
      "\n",
      "Job 4835 has been deleted\n",
      "Looking for job info 4899\n",
      "\n",
      "\n",
      "Not scraped is 112\n",
      "\n",
      "Job 4899 has been deleted\n",
      "4787 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 4925\n",
      "\n",
      "\n",
      "Not scraped is 113\n",
      "\n",
      "Job 4925 has been deleted\n",
      "Looking for job info 4948\n",
      "\n",
      "\n",
      "Not scraped is 114\n",
      "\n",
      "Job 4948 has been deleted\n",
      "Looking for job info 4974\n",
      "\n",
      "\n",
      "Not scraped is 115\n",
      "\n",
      "Job 4974 has been deleted\n",
      "Looking for job info 4997\n",
      "\n",
      "\n",
      "Not scraped is 116\n",
      "\n",
      "Job 4997 has been deleted\n",
      "4883 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 5015\n",
      "\n",
      "\n",
      "Not scraped is 117\n",
      "\n",
      "Job 5015 has been deleted\n",
      "4982 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Looking for job info 5133\n",
      "\n",
      "\n",
      "Not scraped is 118\n",
      "\n",
      "Job 5133 has been deleted\n",
      "5081 jobs have been scraped so far...\n",
      "\n",
      "0  0\n",
      "\n",
      "\\__/\n",
      "\n",
      "Unfortunately, information for 119 jobs was unable to be scraped.\n",
      "\n",
      "Information on a total of 5082 jobs has been scraped\n",
      "\n",
      "\n",
      "Lenght of job_attrs is: 5082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for link in job_links[4100:]: #start at job 212, bc job 1 is index 0\n",
    "    t = random.randint(8,10)\n",
    "    time.sleep(t)\n",
    "    browser = dc.initialize_driver()\n",
    "    browser.get(link)\n",
    "\n",
    "    try:\n",
    "        # wait for page to load\n",
    "        WebDriverWait(browser, 20).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'row'))\n",
    "        )\n",
    "        del_div = browser.find_elements(By.CLASS_NAME, 'row')\n",
    "        # Check if post has been deleted\n",
    "        if del_div[6].text != 'We require users to verify the reCaptcha below to view deleted positions.\\nRelated Searches:\\nBusiness and Financial Services\\nCreate your free job search account\\nReceive new jobs by email\\nPost your resume/CV\\nTrack your applications\\nJoin Now\\nHave an account? Sign in':\n",
    "\n",
    "            job_title = browser.find_element(By.ID, 'jobtitle-header').text\n",
    "            job_loc = browser.find_element(By.CLASS_NAME, 'job-loc').text\n",
    "            div = browser.find_elements(By.ID, 'jobAttrib')\n",
    "            job_attr = div[0].find_element(By.CLASS_NAME, 'job-info').text.split('\\n') # get job data\n",
    "       \n",
    "            div = browser.find_elements(By.ID, 'job')\n",
    "            job_desc = div[0].find_element(By.ID, 'jobDesc').text#.split('\\n')\n",
    "            \n",
    "            job_attr = [job_title] + [job_loc] + job_attr # puts title at the begining of the list\n",
    "            job_attr.append(job_desc) # Append job_description\n",
    "            job_attrs.append(job_attr) # Append job details as list inside another list \n",
    "            \n",
    "            scraped = scraped + 1\n",
    "\n",
    "            browser.quit()\n",
    "            smile = smile +1\n",
    "            if smile == 1:\n",
    "                print('\\nScraping away...\\n')\n",
    "                print('0  0\\n')\n",
    "                print('\\__/\\n')\n",
    "            if smile % 100 == 0:\n",
    "                print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                print('0  0\\n')\n",
    "                print('\\__/\\n')\n",
    "            \n",
    "            # Save progress (every job info)\n",
    "            #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                # Store data (serialize)\n",
    "            with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                pickle.dump(job_attrs, f)\n",
    "\n",
    "        elif IndexError:\n",
    "            print('\\nJob {} has been deleted'.format(smile))\n",
    "            not_scraped = not_scraped + 1\n",
    "\n",
    "            browser.quit()\n",
    "            smile = smile +1\n",
    "            if smile == 1:\n",
    "                print('\\nScraping away...\\n')\n",
    "                print('0  0\\n')\n",
    "                print('\\__/\\n')\n",
    "            if smile % 100 == 0:\n",
    "                print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                print('0  0\\n')\n",
    "                print('\\__/\\n')\n",
    "            \n",
    "            # Save progress (every job info)\n",
    "            #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                # Store data (serialize)\n",
    "            with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                pickle.dump(job_attrs, f)\n",
    "\n",
    "    except Exception: # If cannot find element\n",
    "\n",
    "        # # wait for page to load\n",
    "        # WebDriverWait(browser, 20).until(\n",
    "        #     EC.presence_of_element_located((By.CLASS_NAME, 'iframe'))\n",
    "        # ) \n",
    "               \n",
    "        # See if exception is related to captcha\n",
    "        if browser.find_elements(By.TAG_NAME,'iframe')[0].text.startswith('Request unsuccessful.'):\n",
    "            \n",
    "            #bypassFullCaptcha()\n",
    "            bypassCaptcha1()\n",
    "            \n",
    "            # Check if we get second captcha check\n",
    "            if browser.find_elements(By.TAG_NAME,'iframe')[0].text.startswith('Request unsuccessful.'):\n",
    "                \n",
    "                bypassCaptcha2()\n",
    "                time.sleep(10)\n",
    "        \n",
    "        try:\n",
    "            print('Looking for job info {}\\n'.format(smile))\n",
    "            print('\\nNot scraped is', not_scraped)\n",
    "            # Successfully moved out of Captcha\n",
    "\n",
    "            # wait for page to load\n",
    "            WebDriverWait(browser, 20).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'row'))\n",
    "            )\n",
    "            del_div = browser.find_elements(By.CLASS_NAME, 'row')\n",
    "            # Check if post has been deleted\n",
    "            if del_div[6].text != 'We require users to verify the reCaptcha below to view deleted positions.\\nRelated Searches:\\nBusiness and Financial Services\\nCreate your free job search account\\nReceive new jobs by email\\nPost your resume/CV\\nTrack your applications\\nJoin Now\\nHave an account? Sign in':\n",
    "\n",
    "                job_title = browser.find_element(By.ID, 'jobtitle-header').text\n",
    "                job_loc = browser.find_element(By.CLASS_NAME, 'job-loc').text\n",
    "                div = browser.find_elements(By.ID, 'jobAttrib')\n",
    "                job_attr = div[0].find_element(By.CLASS_NAME, 'job-info').text.split('\\n') # get job data\n",
    "                        #job_attr = [job_title] + job_attr\n",
    "                div = browser.find_elements(By.ID, 'job')\n",
    "                job_desc = div[0].find_element(By.ID, 'jobDesc').text#.split('\\n')\n",
    "                    #job_attr.append(job_desc)\n",
    "                    #job_attrs.append(job_attr)\n",
    "                \n",
    "            #else: # if post is delete\n",
    "                job_attr = [job_title] + [job_loc] + job_attr # puts title at the begining of the list\n",
    "                job_attr.append(job_desc) # Append job_description\n",
    "                job_attrs.append(job_attr) # Append job details as list inside another list \n",
    "                \n",
    "                scraped = scraped + 1\n",
    "                \n",
    "                browser.quit()\n",
    "                smile = smile +1\n",
    "                if smile == 1:\n",
    "                    print('\\nScraping away...\\n')\n",
    "                    print('0  0\\n')\n",
    "                    print('\\__/\\n')\n",
    "                if smile % 100 == 0:\n",
    "                    print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                    print('0  0\\n')\n",
    "                    print('\\__/\\n')\n",
    "                \n",
    "                # Save progress (every job info)\n",
    "                #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                    # Store data (serialize)\n",
    "                with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(job_attrs, f)\n",
    "\n",
    "            elif IndexError:\n",
    "                print('\\nJob {} has been deleted'.format(smile))\n",
    "                not_scraped = not_scraped + 1\n",
    "\n",
    "                browser.quit()\n",
    "                smile = smile +1\n",
    "                if smile == 1:\n",
    "                    print('\\nScraping away...\\n')\n",
    "                    print('0  0\\n')\n",
    "                    print('\\__/\\n')\n",
    "                if smile % 100 == 0:\n",
    "                    print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                    print('0  0\\n')\n",
    "                    print('\\__/\\n')\n",
    "                \n",
    "                # Save progress (every job info)\n",
    "                #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                    # Store data (serialize)\n",
    "                with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(job_attrs, f)\n",
    "            \n",
    "        except Exception:\n",
    "\n",
    "            try:\n",
    "                if del_div[6].text.startswith('We require users to verify the reCaptcha below to view deleted positions'):\n",
    "                    \n",
    "                    print('\\nJob {} has been deleted'.format(smile))\n",
    "                    not_scraped = not_scraped + 1\n",
    "\n",
    "                    browser.quit()\n",
    "                    smile = smile +1\n",
    "                    if smile == 1:\n",
    "                        print('\\nScraping away...\\n')\n",
    "                        print('0  0\\n')\n",
    "                        print('\\__/\\n')\n",
    "                    if smile % 100 == 0:\n",
    "                        print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                        print('0  0\\n')\n",
    "                        print('\\__/\\n')\n",
    "                    \n",
    "                    # Save progress (every job info)\n",
    "                    #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                        # Store data (serialize)\n",
    "                    with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                        pickle.dump(job_attrs, f)\n",
    "            \n",
    "            except Exception:\n",
    "            # If exception is not b/c job is deleted then we need to pause\n",
    "                # API limit exceeded and need to wait, pause 20 minutes and try again\n",
    "                browser.quit()\n",
    "                print('Pausing 20 minutes, buster API overused!')\n",
    "                time.sleep(1200) # wait 20 minutes :( looooooong\n",
    "\n",
    "                pp = 0\n",
    "                aa = 0\n",
    "                while pp < 1:\n",
    "                    try:\n",
    "                        browser = dc.initialize_driver()\n",
    "                        browser.get(link)\n",
    "\n",
    "                        #browser.switch_to.default_content()\n",
    "                        \n",
    "                        if browser.find_elements(By.TAG_NAME,'iframe')[0].text.startswith('Request unsuccessful.'):\n",
    "                            print('Trying again on job ', smile)\n",
    "\n",
    "                            time.sleep(10)\n",
    "\n",
    "                            bypassCaptcha1()\n",
    "\n",
    "                            bypassCaptcha2()\n",
    "                            time.sleep(10)\n",
    "\n",
    "                            print('Looking for job info {}\\n'.format(smile))\n",
    "                            print('\\nNot scraped is', not_scraped)\n",
    "                                \n",
    "                            # Successfully moved out of Captcha\n",
    "\n",
    "                            # wait for page to load\n",
    "                            WebDriverWait(browser, 20).until(\n",
    "                                EC.presence_of_element_located((By.CLASS_NAME, 'row'))\n",
    "                            )\n",
    "                            del_div = browser.find_elements(By.CLASS_NAME, 'row')\n",
    "                            # Check if post has been deleted\n",
    "                            if del_div[6].text != 'We require users to verify the reCaptcha below to view deleted positions.\\nRelated Searches:\\nBusiness and Financial Services\\nCreate your free job search account\\nReceive new jobs by email\\nPost your resume/CV\\nTrack your applications\\nJoin Now\\nHave an account? Sign in':\n",
    "\n",
    "                                job_title = browser.find_element(By.ID, 'jobtitle-header').text\n",
    "                                job_loc = browser.find_element(By.CLASS_NAME, 'job-loc').text\n",
    "                                div = browser.find_elements(By.ID, 'jobAttrib')\n",
    "                                job_attr = div[0].find_element(By.CLASS_NAME, 'job-info').text.split('\\n') # get job data\n",
    "                                        #job_attr = [job_title] + job_attr\n",
    "                                div = browser.find_elements(By.ID, 'job')\n",
    "                                job_desc = div[0].find_element(By.ID, 'jobDesc').text#.split('\\n')\n",
    "                                    #job_attr.append(job_desc)\n",
    "                                    #job_attrs.append(job_attr)\n",
    "                                \n",
    "                            #else: # if post is delete\n",
    "                                job_attr = [job_title] + [job_loc] + job_attr # puts title at the begining of the list\n",
    "                                job_attr.append(job_desc) # Append job_description\n",
    "                                job_attrs.append(job_attr) # Append job details as list inside another list \n",
    "                                \n",
    "                                scraped = scraped + 1\n",
    "                                \n",
    "                                browser.quit()\n",
    "                                smile = smile +1\n",
    "                                if smile == 1:\n",
    "                                    print('\\nScraping away...\\n')\n",
    "                                    print('0  0\\n')\n",
    "                                    print('\\__/\\n')\n",
    "                                if smile % 100 == 0:\n",
    "                                    print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                                    print('0  0\\n')\n",
    "                                    print('\\__/\\n')\n",
    "                                \n",
    "                                # Save progress (every job info)\n",
    "                                #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                                    # Store data (serialize)\n",
    "                                with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                                    pickle.dump(job_attrs, f)\n",
    "                                pp = 1\n",
    "\n",
    "                            elif IndexError:\n",
    "                                print('\\nJob {} has been deleted'.format(smile))\n",
    "                                not_scraped = not_scraped + 1\n",
    "\n",
    "                                browser.quit()\n",
    "                                smile = smile +1\n",
    "                                if smile == 1:\n",
    "                                    print('\\nScraping away...\\n')\n",
    "                                    print('0  0\\n')\n",
    "                                    print('\\__/\\n')\n",
    "                                if smile % 100 == 0:\n",
    "                                    print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                                    print('0  0\\n')\n",
    "                                    print('\\__/\\n')\n",
    "                                \n",
    "                                # Save progress (every job info)\n",
    "                                #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                                    # Store data (serialize)\n",
    "                                with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                                    pickle.dump(job_attrs, f)\n",
    "                                pp = 1\n",
    "                            \n",
    "                                \n",
    "                    except Exception:\n",
    "                        time.sleep(60)\n",
    "                        try:\n",
    "                            if del_div[6].text.startswith('We require users to verify the reCaptcha below to view deleted positions'):\n",
    "                    \n",
    "                                print('\\nJob {} has been deleted'.format(smile))\n",
    "                                not_scraped = not_scraped + 1\n",
    "\n",
    "                                browser.quit()\n",
    "                                smile = smile +1\n",
    "                                if smile == 1:\n",
    "                                    print('\\nScraping away...\\n')\n",
    "                                    print('0  0\\n')\n",
    "                                    print('\\__/\\n')\n",
    "                                if smile % 100 == 0:\n",
    "                                    print(\"{} jobs have been scraped so far...\\n\".format(scraped))\n",
    "                                    print('0  0\\n')\n",
    "                                    print('\\__/\\n')\n",
    "                                \n",
    "                                # Save progress (every job info)\n",
    "                                #if smile % 500 == 0: # Every 500 jobs save as pickle\n",
    "                                    # Store data (serialize)\n",
    "                                with open('Job_Attrs_'+category+'.pkl', 'wb') as f:\n",
    "                                    pickle.dump(job_attrs, f)\n",
    "                                pp = 1\n",
    "                        \n",
    "                        except Exception:\n",
    "\n",
    "                            browser.implicitly_wait(7)\n",
    "                            browser.quit()\n",
    "                            pp = 0\n",
    "                            aa = aa + 1\n",
    "                            print('\\naa is \\n', aa)\n",
    "                            if aa == 5: #Let it try a few times\n",
    "                                print('Pausing for 30 minutes')\n",
    "                                time.sleep(1800) \n",
    "                            \n",
    "                            if aa == 6:\n",
    "                                print('Pausing for 3 hours!')\n",
    "                                time.sleep(10800)\n",
    "                            \n",
    "                            if aa > 6:\n",
    "                                print('Pause for 2 min intervals until it works')\n",
    "                                time.sleep(120)\n",
    "                            \n",
    "                            if aa == 12:\n",
    "                                print('Pause for 3 hours')\n",
    "                                time.sleep(10800)\n",
    "\n",
    "                    \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        # else: # if exception not related to captcha, than simply info doesn't exist\n",
    "        #     not_scraped = not_scraped + 1\n",
    "\n",
    "if not_scraped > 0:\n",
    "\n",
    "    if not_scraped == 1:\n",
    "        print('Unfortunately, information for {} job was unable to be scraped.'.format(not_scraped))\n",
    "    \n",
    "    elif not_scraped > 1:\n",
    "        print('Unfortunately, information for {} jobs was unable to be scraped.'.format(not_scraped))\n",
    "\n",
    "print(\"\\nInformation on a total of {} jobs has been scraped\\n\".format(scraped))\n",
    "\n",
    "print(\"\\nLenght of job_attrs is: {}\\n\".format(len(job_attrs)))\n",
    "\n",
    "# Done scrpaing job attributes\n",
    "\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Student Success Coordinator',\n",
       " 'Williamsburg, KY',\n",
       " 'Type: Full-Time',\n",
       " 'Posted: 04/04/2023',\n",
       " 'Category: Academic Advising',\n",
       " 'Job Type: Full time\\nJob Number: R1281\\nLocation: Williamsburg, KY\\nGet Set for Cumberlands!\\nJoin our team of student-focused individuals working together in order to encourage intellectual and spiritual growth, leadership and service.\\nCURRENT STUDENT or EMPLOYEE? Please log into Workday and use the Career Tile to find and apply to jobs from our internal career sites.\\nJob Responsibilities:\\nProvide academic advising for executive graduate students in multiple academic programs.\\nEffectively communicate in a timely manner with graduate students.\\nWork with the Registrar, International Graduate Admissions, and Department Chairs to continue the flow process of transcript evaluation, transfer credit evaluation, admissions, and academic advising.\\nRespond efficiently to emails and phone calls from academic advisees.\\nComplete, maintain, and update academic planned programs.\\nComplete course registration and academic planning with assigned students.\\nComplete any other related administrative tasks, as assigned. \\nAssist with discipline-specific oversight in the graduate programs. \\nWork with Department Chairs to manage discipline specific academic issues. \\nGain an understanding of the international graduate student admissions process to more efficiently serve the international graduate students.\\nJob Requirements:\\nCompetence in all basic Microsoft Office software required.\\nCurriculum and/or advising experience preferred.\\nBachelor\\'s degree required.\\nCumberlands is different by design. Our employees exemplify our motto in the pursuit of a\\n\"life-more-abundant.\"']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_attrs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10b88bf028bfa9b25f5dbe99d4a0bd4730b6baebc1bad2b22718095ba5dd32bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
